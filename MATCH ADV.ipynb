{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08fb3882-31c5-4e1f-af0e-e4bb22f0aebf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c5e74ff-e005-4b1d-a105-1754e18f34fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install pydantic==1.9.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b98767f0-f17c-4cd4-8897-bfa36443547e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f65f7cc7-6007-42b7-bd91-cfb046003170",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -q -U google-genai pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e0ecdf62-641b-4d94-a217-8f7814bbce3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install pandas rapidfuzz gspread gspread-dataframe google-auth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59ce2b4c-bc06-4156-bdad-9f5961f5dc02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "155ca8aa-b882-475d-b8dd-f817d21f02d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from urllib.parse import unquote_plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e6d6f3b-e4ca-4c3c-8e04-ff00c5e6976c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os, re, unicodedata, argparse, glob\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from rapidfuzz import process, fuzz\n",
    "import gspread\n",
    "from gspread_dataframe import get_as_dataframe, set_with_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35155556-8850-4508-8631-4ea1f9c484a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from urllib.parse import unquote_plus\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "254ac65a-ff85-4341-b843-4cef862c0a1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0db2c25d-19c2-4770-af00-2003621c5566",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "credentials = json.loads(unquote_plus(dbutils.secrets.get(scope=\"latam_bi\", key=\"google_private_key\")))\n",
    "\n",
    "scope = ['https://spreadsheets.google.com/feeds','https://www.googleapis.com/auth/drive']\n",
    "\n",
    "gc = gspread.service_account_from_dict(credentials, scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c75dc3a1-47d0-4cb2-b865-51edbdb95ff1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gc.list_spreadsheet_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b548e64c-b6b0-4c30-b99e-aa4c80509d29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "sheet_id = '1NQNtoNNkzWjuc8UHVNcYi2LnH0U860IIl2KSt-8qcpI'   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d4ad3b1-13c5-41ab-a84a-ae3ce4cbb168",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sheet_id_1 = '1mYDxfmlXPMvpEzfPjm5utV62cRZnjXNPc47n47v_jYo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eca6e370-2231-448e-aaa7-e14098ebce9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sh = gc.open_by_key(sheet_id)\n",
    "ws2 = sh.worksheet(\"Listado Anunciantes Estandarizado\")\n",
    "listado_df = pd.DataFrame(ws2.get_all_records(head=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c33c1e8-804e-46d6-ba91-1759b458537d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "nostandar_df = pd.DataFrame(gc.open_by_key(sheet_id_1).sheet1.get_all_records(head=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7d383f5b-3999-4c4c-bde7-ec48059ee099",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "listado_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e77949a0-c46b-492d-b551-d2969f45578b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(nostandar_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "65b67028-7322-44fc-a4ad-6948048eeae0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(listado_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "119eb491-e649-4356-aff5-c08719392c0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "nostandar_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06a33c47-965d-4596-bb1e-48f2dcda96bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "listado_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff81009c-c50c-4107-99ff-06bbb48ef221",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## First Try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6630fbb-ac74-48eb-87ff-57ab7721ace8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# matcher_from_dfs.py\n",
    "# Reqs: pip install pandas rapidfuzz\n",
    "\n",
    "import re, unicodedata\n",
    "import pandas as pd\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "# ---------- Normalización ----------\n",
    "STOPWORDS = r\"\"\"\n",
    "\\b(\n",
    " S\\.?A\\.?S? | S\\.?R\\.?L\\.? | LTDA | LLC | INC | CORP | CO\\.? | SA\\s*DE\\s*CV |\n",
    " COMPANY | GROUP | HOLDINGS? | OFICIAL(?:\\s+STORE)? | OFFICIAL(?:\\s+STORE)? |\n",
    " THE | EL | LA | LOS | LAS | SUCURSAL | TIENDA | STORE\n",
    ")\\b\n",
    "\"\"\"\n",
    "RE_NO_CLASS = re.compile(r\"\\bno[- _]*classified\\b\", re.IGNORECASE)\n",
    "\n",
    "def norm(s: str) -> str:\n",
    "    s = \"\" if s is None else str(s)\n",
    "    s = unicodedata.normalize(\"NFKD\", s).encode(\"ASCII\",\"ignore\").decode()\n",
    "    s = s.upper().replace(\"&\",\" AND \")\n",
    "    s = re.sub(r\"[^\\w\\s]\", \" \", s)\n",
    "    s = re.sub(STOPWORDS, \" \", s, flags=re.VERBOSE)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def _pick_col(df: pd.DataFrame, exact_pref=(), contains_any=()):\n",
    "    cols = list(df.columns)\n",
    "    for want in exact_pref:\n",
    "        for c in cols:\n",
    "            if str(c).strip().lower() == want.lower():\n",
    "                return c\n",
    "    for c in cols:\n",
    "        lc = str(c).lower()\n",
    "        if any(k.lower() in lc for k in contains_any):\n",
    "            return c\n",
    "    return cols[0] if cols else None\n",
    "\n",
    "# ---------- Preparación del listado ----------\n",
    "def prep_listado(listado_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Asegura columnas MARCA | MARCA_LIMPIA | ANUNCIANTE + alias_norm.\"\"\"\n",
    "    df = listado_df.copy()\n",
    "    # detectar columnas si no están exactas\n",
    "    col_marca  = \"MARCA\" if \"MARCA\" in df.columns else _pick_col(df, exact_pref=(\"MARCA\",), contains_any=(\"marca\",))\n",
    "    col_limpia = \"Marca Limpio\" if \"Marca Limpio\" in df.columns else _pick_col(df, exact_pref=(\"Marca Limpio\",\"Marca limpia\"), contains_any=(\"limpio\",\"limpia\",\"estandar\"))\n",
    "    col_anunc  = \"Anunciante\" if \"Anunciante\" in df.columns else _pick_col(df, exact_pref=(\"Anunciante\",), contains_any=(\"anunc\",))\n",
    "\n",
    "    df = df[[col_marca, col_limpia, col_anunc]].dropna(how=\"all\").copy()\n",
    "    df.columns = [\"MARCA\",\"MARCA_LIMPIA\",\"ANUNCIANTE\"]\n",
    "    df[\"alias_norm\"] = df[\"MARCA\"].map(norm)\n",
    "    df = df[df[\"alias_norm\"]!=\"\"].drop_duplicates(\"alias_norm\")\n",
    "    return df\n",
    "\n",
    "# ---------- Matching desde DataFrames ----------\n",
    "def build_mapping_from_dfs(nostandar_df: pd.DataFrame,\n",
    "                           listado_preparado: pd.DataFrame,\n",
    "                           csv_col: str = None,\n",
    "                           thresh: int = 90) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retorna mapping: csv_marca | csv_count | match_type | score | listado_marca | marca_limpia | anunciante | decision\n",
    "    - Descarta 'no classified'\n",
    "    - Exacto por norm, luego fuzzy token_set_ratio con umbral 'thresh'\n",
    "    \"\"\"\n",
    "    df_csv = nostandar_df.copy()\n",
    "\n",
    "    # detectar columna de marca del CSV si no la pasan\n",
    "    if not csv_col or csv_col not in df_csv.columns:\n",
    "        if \"Marca\" in df_csv.columns: csv_col = \"Marca\"\n",
    "        elif \"ADVERTISER_ADMETRICS_STR\" in df_csv.columns: csv_col = \"ADVERTISER_ADMETRICS_STR\"\n",
    "        else: csv_col = _pick_col(df_csv, contains_any=(\"marca\",\"advertiser\",\"brand\"))\n",
    "\n",
    "    # filtrar no classified\n",
    "    keep = ~df_csv[csv_col].astype(str).str.contains(RE_NO_CLASS, na=False)\n",
    "    df_csv = df_csv.loc[keep].copy()\n",
    "\n",
    "    # únicos + conteo por normalización\n",
    "    s = df_csv[csv_col].astype(str).str.strip()\n",
    "    tmp = pd.DataFrame({\"csv_marca\": s})\n",
    "    tmp[\"csv_norm\"] = tmp[\"csv_marca\"].map(norm)\n",
    "    grp = tmp.groupby(\"csv_norm\")[\"csv_marca\"].agg([\"first\",\"size\"]).reset_index()\n",
    "    grp.columns = [\"csv_norm\",\"csv_marca\",\"csv_count\"]\n",
    "\n",
    "    alias_index = listado_preparado.set_index(\"alias_norm\")\n",
    "    alias_norms = alias_index.index.tolist()\n",
    "\n",
    "    rows = []\n",
    "    for _, r in grp.iterrows():\n",
    "        n, raw, cnt = r[\"csv_norm\"], r[\"csv_marca\"], int(r[\"csv_count\"])\n",
    "        # exacto\n",
    "        if n in alias_index.index:\n",
    "            rec = alias_index.loc[n]\n",
    "            rows.append([raw, cnt, \"exact\", 100, rec[\"MARCA\"], rec[\"MARCA_LIMPIA\"], rec[\"ANUNCIANTE\"]])\n",
    "            continue\n",
    "        # fuzzy\n",
    "        best = process.extractOne(n, alias_norms, scorer=fuzz.token_set_ratio) if alias_norms else None\n",
    "        if best and best[1] >= thresh:\n",
    "            rec = alias_index.loc[best[0]]\n",
    "            rows.append([raw, cnt, \"fuzzy\", int(best[1]), rec[\"MARCA\"], rec[\"MARCA_LIMPIA\"], rec[\"ANUNCIANTE\"]])\n",
    "        else:\n",
    "            rows.append([raw, cnt, \"none\", int(best[1] if best else 0), \"\", \"\", \"\"])\n",
    "\n",
    "    out = pd.DataFrame(rows, columns=[\n",
    "        \"csv_marca\",\"csv_count\",\"match_type\",\"score\",\"listado_marca\",\"marca_limpia\",\"anunciante\"\n",
    "    ]).sort_values([\"match_type\",\"score\",\"csv_count\"], ascending=[True, False, False])\n",
    "    out[\"decision\"] = out.apply(\n",
    "        lambda r: \"COINCIDE\" if r[\"match_type\"] in (\"exact\",\"fuzzy\") and r[\"score\"]>=thresh else \"NUEVO/REVISAR\",\n",
    "        axis=1\n",
    "    )\n",
    "    return out\n",
    "\n",
    "# ---------- Aprendizaje SOLO de coincidencias fuzzy (o según el modo) ----------\n",
    "def learn_into_listado(listado_preparado: pd.DataFrame,\n",
    "                       mapping_df: pd.DataFrame,\n",
    "                       learn_mode: str = \"fuzzy_only\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Devuelve un listado_df ACTUALIZADO (en memoria).\n",
    "    - fuzzy_only: agrega solo coincidencias fuzzy (no exactas)\n",
    "    - exact_only: agrega solo exactas\n",
    "    - all: agrega exactas + fuzzy\n",
    "    No agrega 'none'. Evita duplicados por alias_norm.\n",
    "    \"\"\"\n",
    "    if learn_mode == \"fuzzy_only\":\n",
    "        to_learn = mapping_df.query(\"decision=='COINCIDE' and match_type=='fuzzy'\")[[\"csv_marca\",\"marca_limpia\",\"anunciante\"]].copy()\n",
    "    elif learn_mode == \"exact_only\":\n",
    "        to_learn = mapping_df.query(\"decision=='COINCIDE' and match_type=='exact'\")[[\"csv_marca\",\"marca_limpia\",\"anunciante\"]].copy()\n",
    "    else:  # all\n",
    "        to_learn = mapping_df.query(\"decision=='COINCIDE'\")[[\"csv_marca\",\"marca_limpia\",\"anunciante\"]].copy()\n",
    "\n",
    "    if to_learn.empty:\n",
    "        return listado_preparado.copy()\n",
    "\n",
    "    to_learn = to_learn.rename(columns={\n",
    "        \"csv_marca\": \"MARCA\",\n",
    "        \"marca_limpia\": \"MARCA_LIMPIA\",\n",
    "        \"anunciante\": \"ANUNCIANTE\"\n",
    "    })\n",
    "    to_learn[\"alias_norm\"] = to_learn[\"MARCA\"].map(norm)\n",
    "\n",
    "    base = listado_preparado.copy()\n",
    "    base = pd.concat([base, to_learn], ignore_index=True)\n",
    "    base = base.sort_values([\"ANUNCIANTE\",\"MARCA_LIMPIA\",\"MARCA\"], na_position=\"last\")\n",
    "    base = base.drop_duplicates(\"alias_norm\", keep=\"first\").reset_index(drop=True)\n",
    "    return base\n",
    "\n",
    "# ---------- Entry point para trabajar EN MEMORIA ----------\n",
    "def run_from_dfs(nostandar_df: pd.DataFrame,\n",
    "                 listado_df: pd.DataFrame,\n",
    "                 csv_col: str = None,\n",
    "                 thresh: int = 90,\n",
    "                 learn: bool = True,\n",
    "                 learn_mode: str = \"fuzzy_only\"):\n",
    "    \"\"\"\n",
    "    Uso:\n",
    "        mapping_df, listado_actualizado = run_from_dfs(nostandar_df, listado_df,\n",
    "                                                       csv_col=\"Marca\",\n",
    "                                                       thresh=90,\n",
    "                                                       learn=True,\n",
    "                                                       learn_mode=\"fuzzy_only\")\n",
    "    \"\"\"\n",
    "    maestro = prep_listado(listado_df)\n",
    "    mapping = build_mapping_from_dfs(nostandar_df, maestro, csv_col=csv_col, thresh=thresh)\n",
    "    if learn:\n",
    "        maestro_upd = learn_into_listado(maestro, mapping, learn_mode=learn_mode)\n",
    "    else:\n",
    "        maestro_upd = maestro\n",
    "    return mapping, maestro_upd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31c9c869-640f-4aca-b2a8-61d0b1554a74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# suponiendo que ya tenés:\n",
    "# - nostandar_df: DataFrame del CSV (col 'Marca' o 'ADVERTISER_ADMETRICS_STR')\n",
    "# - listado_df: DataFrame maestro con columnas 'MARCA', 'Marca Limpio', 'Anunciante'\n",
    "\n",
    "mapping_df, listado_actualizado = run_from_dfs(\n",
    "    nostandar_df,\n",
    "    listado_df,\n",
    "    csv_col=\"Marca\",          # o \"ADVERTISER_ADMETRICS_STR\"\n",
    "    thresh=90,\n",
    "    learn=True,\n",
    "    learn_mode=\"fuzzy_only\"   # solo incorpora coincidencias NO exactas\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e5f6a96-1647-44b4-960c-6735f0e3cced",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mapping_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b670cc88-f7d9-426f-b64f-a3d3fe2a1761",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "listado_actualizado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "889d9dcb-f339-4fbe-98a6-a834403d0868",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Last Try\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c30c3ebb-fa03-41f2-819a-3f5ccd3a0a7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DF → DF (sin I/O). Reqs: pandas, rapidfuzz\n",
    "import re, unicodedata\n",
    "import pandas as pd\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "# --- normalización y filtros ---\n",
    "STOPWORDS = r\"\"\"\n",
    "\\b(\n",
    " S\\.?A\\.?S? | S\\.?R\\.?L\\.? | LTDA | LLC | INC | CORP | CO\\.? | SA\\s*DE\\s*CV |\n",
    " COMPANY | GROUP | HOLDINGS? | OFICIAL(?:\\s+STORE)? | OFFICIAL(?:\\s+STORE)? |\n",
    " THE | EL | LA | LOS | LAS | SUCURSAL | TIENDA | STORE\n",
    ")\\b\n",
    "\"\"\"\n",
    "EXCLUDE_NOTCLASS = re.compile(\n",
    "    r\"(?:\\b(?:no|not)[- _]*classified\\b)|\\bunclassified\\b|\"\n",
    "    r\"\\bno[- _]*clasificado\\b|\\bsin[- _]*clasificar\\b|\"\n",
    "    r\"\\bnot[- _]*set\\b|\\bundefined\\b|\\bN/?A\\b\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "def norm(s: str) -> str:\n",
    "    s = \"\" if s is None else str(s)\n",
    "    s = unicodedata.normalize(\"NFKD\", s).encode(\"ASCII\",\"ignore\").decode()\n",
    "    s = s.upper().replace(\"&\",\" AND \")\n",
    "    s = re.sub(r\"[^\\w\\s]\", \" \", s)\n",
    "    s = re.sub(STOPWORDS, \" \", s, flags=re.VERBOSE)\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "def _tokens(s: str) -> set: return set(t for t in re.split(r\"\\s+\", s) if t)\n",
    "def _jacc(a: set, b: set) -> float:\n",
    "    return len(a & b) / len(a | b) if a and b and (a|b) else 0.0\n",
    "\n",
    "def _pick(df, *names):\n",
    "    cols = list(df.columns)\n",
    "    for w in names:\n",
    "        for c in cols:\n",
    "            if str(c).strip().lower() == w.lower(): return c\n",
    "    for w in names:\n",
    "        for c in cols:\n",
    "            if w.lower() in str(c).lower(): return c\n",
    "    return None\n",
    "\n",
    "# --- preparar listado ---\n",
    "def prep_listado(listado_df: pd.DataFrame):\n",
    "    cm = \"MARCA\" if \"MARCA\" in listado_df.columns else _pick(listado_df, \"MARCA\", \"marca\")\n",
    "    cl = \"Marca Limpio\" if \"Marca Limpio\" in listado_df.columns else _pick(listado_df, \"Marca Limpio\",\"Marca limpia\",\"limpio\",\"limpia\")\n",
    "    ca = \"Anunciante\" if \"Anunciante\" in listado_df.columns else _pick(listado_df, \"Anunciante\",\"anunciante\")\n",
    "    base = listado_df[[cm, cl, ca]].copy()\n",
    "    base.columns = [\"MARCA\",\"MARCA_LIMPIA\",\"ANUNCIANTE\"]\n",
    "    base[\"alias_norm\"]  = base[\"MARCA\"].map(norm)\n",
    "    base[\"limpia_norm\"] = base[\"MARCA_LIMPIA\"].map(norm)\n",
    "    base[\"anunc_norm\"]  = base[\"ANUNCIANTE\"].map(norm)\n",
    "    base = base[base[\"alias_norm\"]!=\"\"].drop_duplicates(\"alias_norm\")\n",
    "    return {\n",
    "        \"df\": base,\n",
    "        \"brand_norms\": set(pd.unique(pd.concat([base[\"alias_norm\"], base[\"limpia_norm\"]])))-{\"\"},\n",
    "        \"anunc_norms\": set(pd.unique(base[\"anunc_norm\"]))-({\"\"}),\n",
    "        \"idx_alias\":  base.drop_duplicates(\"alias_norm\").set_index(\"alias_norm\"),\n",
    "        \"idx_limpia\": base.drop_duplicates(\"limpia_norm\").set_index(\"limpia_norm\"),\n",
    "        \"idx_anunc\":  base.drop_duplicates(\"anunc_norm\").set_index(\"anunc_norm\"),\n",
    "    }\n",
    "\n",
    "# --- matching: BRAND primero, fallback ANUNCIANTE ---\n",
    "def build_mapping_brand_first(nostandar_df: pd.DataFrame, L: dict,\n",
    "                              thresh: int = 90, require_token_overlap: bool = True) -> pd.DataFrame:\n",
    "    need = [\"ADVERTISER_ADMETRICS_STR\",\"BRAND_ADMETRICS_STR\",\"BRAND_ID_FLT\",\"RAW_BRAND_STR\",\n",
    "            \"BRAND_STR\",\"LEGAL_NAME_STR\",\"ADVERTISER_ID_FLT\",\"ADVERTISER_STR\"]\n",
    "    base = nostandar_df.copy()\n",
    "    for c in need:\n",
    "        if c not in base.columns: base[c] = \"\"\n",
    "\n",
    "    # excluir basura\n",
    "    keep = ~base[\"BRAND_ADMETRICS_STR\"].astype(str).str.contains(EXCLUDE_NOTCLASS, na=False)\n",
    "    keep &= ~base[\"ADVERTISER_STR\"].astype(str).str.contains(EXCLUDE_NOTCLASS, na=False)\n",
    "    base = base[keep].copy()\n",
    "\n",
    "    # norms\n",
    "    base[\"csv_brand\"]      = base[\"BRAND_ADMETRICS_STR\"].astype(str).str.strip()\n",
    "    base[\"csv_brand_norm\"] = base[\"csv_brand\"].map(norm)\n",
    "    base[\"csv_adv\"] = base[\"ADVERTISER_STR\"].astype(str).where(\n",
    "        base[\"ADVERTISER_STR\"].astype(str).str.len()>0,\n",
    "        base[\"ADVERTISER_ADMETRICS_STR\"].astype(str)\n",
    "    ).str.strip()\n",
    "    base[\"csv_adv_norm\"] = base[\"csv_adv\"].map(norm)\n",
    "\n",
    "    grp = (base.groupby(\"csv_brand_norm\")\n",
    "              .agg(advertiser_admetrics=(\"ADVERTISER_ADMETRICS_STR\",\"first\"),\n",
    "                   admetrics_brand=(\"BRAND_ADMETRICS_STR\",\"first\"),\n",
    "                   advertiser_str=(\"ADVERTISER_STR\",\"first\"),\n",
    "                   brand_str=(\"BRAND_STR\",\"first\"),\n",
    "                   legal_name=(\"LEGAL_NAME_STR\",\"first\"),\n",
    "                   raw_brand=(\"RAW_BRAND_STR\",\"first\"),\n",
    "                   advertiser_id=(\"ADVERTISER_ID_FLT\",\"first\"),\n",
    "                   brand_id=(\"BRAND_ID_FLT\",\"first\"),\n",
    "                   csv_brand=(\"csv_brand\",\"first\"),\n",
    "                   csv_adv=(\"csv_adv\",\"first\"),\n",
    "                   csv_adv_norm=(\"csv_adv_norm\",\"first\"),\n",
    "                   count_in_csv=(\"csv_brand\",\"size\")).reset_index())\n",
    "\n",
    "    idxA, idxL, idxN = L[\"idx_alias\"], L[\"idx_limpia\"], L[\"idx_anunc\"]\n",
    "    brand_norms = list(L[\"brand_norms\"]); anunc_norms = list(L[\"anunc_norms\"])\n",
    "\n",
    "    rows=[]\n",
    "    for _, r in grp.iterrows():\n",
    "        n_brand = r[\"csv_brand_norm\"]; cnt = int(r[\"count_in_csv\"])\n",
    "        rec=None; mtype=None; score=0; matched_alias=None\n",
    "\n",
    "        # 1) marca exacta (MARCA o Marca Limpio)\n",
    "        if n_brand in idxA.index:\n",
    "            rec=idxA.loc[n_brand]; mtype=\"exact\"; score=100; matched_alias=n_brand\n",
    "        elif n_brand in idxL.index:\n",
    "            rec=idxL.loc[n_brand]; mtype=\"exact\"; score=100; matched_alias=n_brand\n",
    "        else:\n",
    "            # fuzzy sobre universo de marcas\n",
    "            best_norm=None; s=0\n",
    "            if brand_norms:\n",
    "                best = process.extractOne(n_brand, brand_norms, scorer=fuzz.token_set_ratio)\n",
    "                if best: best_norm, s, _ = best\n",
    "            accept=False\n",
    "            if best_norm and s>=thresh:\n",
    "                t_in, t_best = _tokens(n_brand), _tokens(best_norm)\n",
    "                accept = (len(t_in & t_best) >= 2) or (_jacc(t_in, t_best) >= 0.5) if require_token_overlap else True\n",
    "            if accept:\n",
    "                rec = idxA.loc[best_norm] if best_norm in idxA.index else idxL.loc[best_norm]\n",
    "                mtype=\"fuzzy\"; score=int(s); matched_alias=best_norm\n",
    "\n",
    "        if rec is not None:\n",
    "            rows.append([\n",
    "                r[\"advertiser_admetrics\"], r[\"admetrics_brand\"], r[\"advertiser_str\"], r[\"brand_str\"],\n",
    "                r[\"legal_name\"], r[\"raw_brand\"], r[\"advertiser_id\"], r[\"brand_id\"], cnt,\n",
    "                \"brand\", mtype, score,\n",
    "                rec[\"MARCA\"], rec[\"MARCA_LIMPIA\"], rec[\"ANUNCIANTE\"],\n",
    "                r[\"csv_brand_norm\"], r[\"csv_adv_norm\"], matched_alias, None,\n",
    "                (mtype==\"fuzzy\"), \"COINCIDE\"\n",
    "            ])\n",
    "            continue\n",
    "\n",
    "        # 2) fallback por anunciante\n",
    "        n_adv = r[\"csv_adv_norm\"]; adv_rec=None; mtypeA=None; sA=0; matched_anunc=None\n",
    "        if n_adv in idxN.index:\n",
    "            adv_rec=idxN.loc[n_adv]; mtypeA=\"exact\"; sA=100; matched_anunc=n_adv\n",
    "        else:\n",
    "            best_adv=None; s2=0\n",
    "            if anunc_norms:\n",
    "                best = process.extractOne(n_adv, anunc_norms, scorer=fuzz.token_set_ratio)\n",
    "                if best: best_adv, s2, _ = best\n",
    "            accept=False\n",
    "            if best_adv and s2>=thresh:\n",
    "                t_in, t_best = _tokens(n_adv), _tokens(best_adv)\n",
    "                accept = (len(t_in & t_best) >= 2) or (_jacc(t_in, t_best) >= 0.5) if require_token_overlap else True\n",
    "            if accept:\n",
    "                adv_rec=idxN.loc[best_adv]; mtypeA=\"fuzzy\"; sA=int(s2); matched_anunc=best_adv\n",
    "\n",
    "        if adv_rec is not None:\n",
    "            rows.append([\n",
    "                r[\"advertiser_admetrics\"], r[\"admetrics_brand\"], r[\"advertiser_str\"], r[\"brand_str\"],\n",
    "                r[\"legal_name\"], r[\"raw_brand\"], r[\"advertiser_id\"], r[\"brand_id\"], cnt,\n",
    "                \"advertiser\", mtypeA, sA,\n",
    "                \"\", \"\", adv_rec[\"ANUNCIANTE\"],\n",
    "                r[\"csv_brand_norm\"], r[\"csv_adv_norm\"], None, matched_anunc,\n",
    "                False, \"COINCIDE\"\n",
    "            ])\n",
    "            continue\n",
    "\n",
    "        # 3) sin match\n",
    "        rows.append([\n",
    "            r[\"advertiser_admetrics\"], r[\"admetrics_brand\"], r[\"advertiser_str\"], r[\"brand_str\"],\n",
    "            r[\"legal_name\"], r[\"raw_brand\"], r[\"advertiser_id\"], r[\"brand_id\"], cnt,\n",
    "            None, \"none\", 0,\n",
    "            \"\", \"\", \"\",\n",
    "            r[\"csv_brand_norm\"], r[\"csv_adv_norm\"], None, None,\n",
    "            False, \"NUEVO/REVISAR\"\n",
    "        ])\n",
    "\n",
    "    mapping_df = pd.DataFrame(rows, columns=[\n",
    "        \"advertiser_admetrics\",\"admetrics_brand\",\"advertiser_str\",\"brand_str\",\n",
    "        \"legal_name\",\"raw_brand\",\"advertiser_id\",\"brand_id\",\"count_in_csv\",\n",
    "        \"match_scope\",\"match_type\",\"score\",\n",
    "        \"listado_marca\",\"marca_limpia\",\"anunciante\",\n",
    "        \"csv_brand_norm\",\"csv_adv_norm\",\"matched_alias_norm\",\"matched_anunc_norm\",\n",
    "        \"nueva_variante\",\"decision\"\n",
    "    ])\n",
    "    mapping_df[\"coincidencia\"] = mapping_df[\"match_type\"].map({\"exact\":\"EXACTA\",\"fuzzy\":\"FUZZY\"}).fillna(\"SIN MATCH\")\n",
    "    return mapping_df\n",
    "\n",
    "def ensure_nueva_variante(mdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    m = mdf.copy(); idx = m.index\n",
    "    mscope = m[\"match_scope\"] if \"match_scope\" in m.columns else pd.Series([\"brand\"]*len(idx), index=idx)\n",
    "    mtype  = m[\"match_type\"]  if \"match_type\"  in m.columns else pd.Series([\"\"], index=idx)\n",
    "    decis  = m[\"decision\"]    if \"decision\"    in m.columns else pd.Series([\"\"], index=idx)\n",
    "    m[\"nueva_variante\"] = (\n",
    "        mscope.astype(str).str.lower().eq(\"brand\") &\n",
    "        mtype.astype(str).str.lower().eq(\"fuzzy\") &\n",
    "        decis.astype(str).str.upper().eq(\"COINCIDE\")\n",
    "    )\n",
    "    return m\n",
    "\n",
    "def add_new_aliases_with_flag(listado_df: pd.DataFrame, mapping_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    cm = \"MARCA\" if \"MARCA\" in listado_df.columns else _pick(listado_df, \"MARCA\",\"marca\")\n",
    "    cl = \"Marca Limpio\" if \"Marca Limpio\" in listado_df.columns else _pick(listado_df, \"Marca Limpio\",\"Marca limpia\",\"limpio\",\"limpia\")\n",
    "    ca = \"Anunciante\" if \"Anunciante\" in listado_df.columns else _pick(listado_df, \"Anunciante\",\"anunciante\")\n",
    "\n",
    "    base = listado_df.copy()\n",
    "    base[\"alias_norm\"] = base[cm].map(norm)\n",
    "\n",
    "    m = ensure_nueva_variante(mapping_df)\n",
    "    nuevas = (m.query(\"nueva_variante == True and decision == 'COINCIDE' and match_scope == 'brand'\")\n",
    "                .loc[:, [\"advertiser_admetrics\",\"marca_limpia\",\"anunciante\"]]\n",
    "                .rename(columns={\"advertiser_admetrics\": cm, \"marca_limpia\": cl, \"anunciante\": ca}))\n",
    "    if nuevas.empty:\n",
    "        out = base.copy(); out[\"es_nuevo\"] = False\n",
    "        return out[[cm, cl, ca, \"es_nuevo\"]]\n",
    "\n",
    "    nuevas[\"alias_norm\"] = nuevas[cm].map(norm)\n",
    "    pre = set(base[\"alias_norm\"])\n",
    "    nuevas = nuevas[~nuevas[\"alias_norm\"].isin(pre)]\n",
    "\n",
    "    existentes = base.assign(es_nuevo=False)\n",
    "    agregadas  = nuevas.assign(es_nuevo=True)\n",
    "\n",
    "    out = (pd.concat([existentes, agregadas], ignore_index=True)\n",
    "             .drop_duplicates(\"alias_norm\", keep=\"first\")\n",
    "             .drop(columns=[\"alias_norm\"])\n",
    "             .reset_index(drop=True))\n",
    "    return out[[cm, cl, ca, \"es_nuevo\"]]\n",
    "\n",
    "# --- FUNCIÓN ÚNICA: DF→DF ---\n",
    "def run_from_dfs(nostandar_df: pd.DataFrame, listado_df: pd.DataFrame,\n",
    "                 thresh: int = 90, require_token_overlap: bool = True):\n",
    "    prep = prep_listado(listado_df)\n",
    "    mapping_df = build_mapping_brand_first(nostandar_df, prep, thresh=thresh, require_token_overlap=require_token_overlap)\n",
    "    listado_flag = add_new_aliases_with_flag(listado_df, mapping_df)\n",
    "    return mapping_df, listado_flag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc24a3e3-0063-4583-bfcb-5f94706dce81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ya tenés estos dos DataFrames:\n",
    "\n",
    "mapping_df, listado_flag = run_from_dfs(nostandar_df, listado_df, thresh=90)\n",
    "# listo: mapping_df y listado_flag son DataFrames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a99fe118-3593-4147-8eba-8de80781a97b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mapping_df.to_excel(\"mapping_df.xlsx\", engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c672501b-222d-4926-b455-636bfdc048ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "listado_flag.to_excel(\"listado_flag.xlsx\", engine='openpyxl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3325f5d9-be1a-431c-ab3d-61a0ea775405",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## prueba de los que no estan mappeados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d33da5af-b9d5-4ae6-b1f9-1fc12d3c61c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#ESTE ES EL SUGGESTED, NO NOS INTERESA\n",
    "\n",
    "# Reqs: pandas, rapidfuzz\n",
    "import re, unicodedata\n",
    "import pandas as pd\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "# -------- normalización (misma lógica que venimos usando) --------\n",
    "STOPWORDS = r\"\"\"\\b( S\\.?A\\.?S? | S\\.?R\\.?L\\.? | LTDA | LLC | INC | CORP | CO\\.? | SA\\s*DE\\s*CV |\n",
    " COMPANY | GROUP | HOLDINGS? | THE | EL | LA | LOS | LAS | STORE | TIENDA )\\b\"\"\"\n",
    "EXCLUDE_NOTCLASS = re.compile(\n",
    "    r\"(?:\\b(?:no|not)[- _]*classified\\b)|\\bunclassified\\b|\\bno[- _]*clasificado\\b|\"\n",
    "    r\"\\bsin[- _]*clasificar\\b|\\bnot[- _]*set\\b|\\bundefined\\b|\\bN/?A\\b\", re.IGNORECASE)\n",
    "IGNORED_TOKENS = {\"NOT\",\"CLASSIFIED\",\"UNCLASSIFIED\",\"SIN\",\"CLASIFICAR\",\"NO\",\"SET\",\"UNDEFINED\",\"N\",\"A\",\"NA\"}\n",
    "\n",
    "def _norm(s: str) -> str:\n",
    "    s = \"\" if s is None else str(s)\n",
    "    s = unicodedata.normalize(\"NFKD\", s).encode(\"ASCII\",\"ignore\").decode()\n",
    "    s = s.upper().replace(\"&\", \" AND \")\n",
    "    s = re.sub(r\"[^\\w\\s]\", \" \", s)\n",
    "    s = re.sub(STOPWORDS, \" \", s, flags=re.VERBOSE)\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "def _tokens(s: str) -> set:\n",
    "    return set(t for t in re.split(r\"\\s+\", s) if t)\n",
    "\n",
    "def _coalesce(*vals):\n",
    "    for v in vals:\n",
    "        if isinstance(v, str) and v.strip():\n",
    "            return v.strip()\n",
    "    return \"\"\n",
    "\n",
    "# -------- construye el pool SOLO de BRAND (NUEVO/REVISAR + Not Classified) --------\n",
    "def _build_pool(nostandar_df: pd.DataFrame, mapping_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # a) NUEVO/REVISAR del mapping\n",
    "    m = mapping_df.copy()\n",
    "    nuevos = m[m[\"decision\"].astype(str).str.upper().eq(\"NUEVO/REVISAR\")].copy()\n",
    "    if \"count_in_csv\" not in nuevos.columns:\n",
    "        nuevos[\"count_in_csv\"] = 1\n",
    "    a = pd.DataFrame({\n",
    "        \"ADVERTISER_ADMETRICS_STR\": nuevos.get(\"advertiser_admetrics\",\"\"),\n",
    "        \"BRAND_ADMETRICS_STR\": nuevos.get(\"admetrics_brand\",\"\").fillna(\"\"),\n",
    "        \"count_in_csv\": nuevos[\"count_in_csv\"].fillna(1).astype(int),\n",
    "        \"source\": \"nuevo_revisar\"\n",
    "    })\n",
    "    a = a[a[\"BRAND_ADMETRICS_STR\"].astype(str).str.strip()!=\"\"]\n",
    "\n",
    "    # b) Not Classified del CSV (solo brand)\n",
    "    csv_nc = nostandar_df.copy()\n",
    "    mask_nc = (\n",
    "        csv_nc.get(\"BRAND_ADMETRICS_STR\",\"\").astype(str).str.contains(EXCLUDE_NOTCLASS, na=False) |\n",
    "        csv_nc.get(\"BRAND_STR\",\"\").astype(str).str.contains(EXCLUDE_NOTCLASS, na=False) |\n",
    "        csv_nc.get(\"RAW_BRAND_STR\",\"\").astype(str).str.contains(EXCLUDE_NOTCLASS, na=False)\n",
    "    )\n",
    "    csv_nc = csv_nc[mask_nc].copy()\n",
    "    if not csv_nc.empty:\n",
    "        b = pd.DataFrame({\n",
    "            \"ADVERTISER_ADMETRICS_STR\": csv_nc.get(\"ADVERTISER_ADMETRICS_STR\", csv_nc.get(\"ADVERTISER_STR\",\"\")),\n",
    "            \"BRAND_ADMETRICS_STR\": csv_nc.apply(lambda r: _coalesce(r.get(\"BRAND_ADMETRICS_STR\",\"\"),\n",
    "                                                                     r.get(\"RAW_BRAND_STR\",\"\"),\n",
    "                                                                     r.get(\"BRAND_STR\",\"\")), axis=1),\n",
    "            \"count_in_csv\": 1,\n",
    "            \"source\": \"not_classified\"\n",
    "        })\n",
    "        b = b[b[\"BRAND_ADMETRICS_STR\"].astype(str).str.strip()!=\"\"]\n",
    "    else:\n",
    "        b = pd.DataFrame(columns=[\"ADVERTISER_ADMETRICS_STR\",\"BRAND_ADMETRICS_STR\",\"count_in_csv\",\"source\"])\n",
    "\n",
    "    pool = pd.concat([a,b], ignore_index=True)\n",
    "    pool[\"norm\"] = pool[\"BRAND_ADMETRICS_STR\"].map(_norm)\n",
    "    pool = pool[pool[\"norm\"]!=\"\"].copy()\n",
    "    return pool\n",
    "\n",
    "# -------- clusteriza por parecido y arma Match + anunciante_suggest --------\n",
    "def build_ai_input_table(nostandar_df: pd.DataFrame,\n",
    "                         mapping_df: pd.DataFrame,\n",
    "                         threshold:int=90,\n",
    "                         min_share:float=0.6,\n",
    "                         max_core_tokens:int=3) -> pd.DataFrame:\n",
    "    pool = _build_pool(nostandar_df, mapping_df)\n",
    "    if pool.empty:\n",
    "        return pd.DataFrame(columns=[\"ADVERTISER_ADMETRICS_STR\",\"BRAND_ADMETRICS_STR\",\"Match\",\"anunciante_suggest\"])\n",
    "\n",
    "    # 1) únicos por norm para clusterizar\n",
    "    uniq = (pool.groupby(\"norm\")\n",
    "                 .agg(BRAND_ADMETRICS_STR=(\"BRAND_ADMETRICS_STR\",\"first\"),\n",
    "                      count_in_csv=(\"count_in_csv\",\"sum\"))\n",
    "                 .reset_index()\n",
    "                 .sort_values(\"count_in_csv\", ascending=False)\n",
    "                 .reset_index(drop=True))\n",
    "\n",
    "    # 2) clustering greedy por similitud (token_set_ratio)\n",
    "    n=len(uniq); used=[False]*n; group_id=[-1]*n; gid=0\n",
    "    for i in range(n):\n",
    "        if used[i]: continue\n",
    "        gid+=1; used[i]=True; group_id[i]=gid\n",
    "        head = uniq.at[i,\"norm\"]\n",
    "        for j in range(i+1,n):\n",
    "            if used[j]: continue\n",
    "            if fuzz.token_set_ratio(head, uniq.at[j,\"norm\"]) >= threshold:\n",
    "                used[j]=True; group_id[j]=gid\n",
    "    uniq[\"group_id\"]=group_id\n",
    "\n",
    "    # 3) por grupo: calcular núcleo (Match) y sugerencia\n",
    "    groups=[]\n",
    "    for g, sub in uniq.groupby(\"group_id\"):\n",
    "        # núcleo por intersección/ mayoritarios\n",
    "        token_sets = [(_tokens(s) - IGNORED_TOKENS) for s in sub[\"norm\"]]\n",
    "        core = set.intersection(*token_sets) if token_sets else set()\n",
    "        if not core:\n",
    "            total=len(token_sets); freq={}; appear={}\n",
    "            for k, ts in enumerate(token_sets):\n",
    "                cnt=int(sub.iloc[k][\"count_in_csv\"])\n",
    "                for t in ts: freq[t]=freq.get(t,0)+cnt\n",
    "                for t in ts: appear[t]=appear.get(t,0)+1\n",
    "            majority=[t for t,m in appear.items() if m/total >= min_share]\n",
    "            core=set(sorted(majority, key=lambda t: freq.get(t,0), reverse=True)[:max_core_tokens])\n",
    "\n",
    "        match_str = \" \".join(sorted(core)).title() if core else sub.iloc[0][\"BRAND_ADMETRICS_STR\"].title()\n",
    "\n",
    "        # sugerencia = variante más frecuente del grupo\n",
    "        top_idx = sub.sort_values(\"count_in_csv\", ascending=False).index[0]\n",
    "        suggest = uniq.loc[top_idx, \"BRAND_ADMETRICS_STR\"].title()\n",
    "\n",
    "        groups.append({\"group_id\": g, \"Match\": match_str, \"anunciante_suggest\": suggest})\n",
    "\n",
    "    grp_info = pd.DataFrame(groups)\n",
    "\n",
    "    # 4) mapear grupo a cada fila original del pool\n",
    "    pool = pool.merge(uniq[[\"norm\",\"group_id\"]], on=\"norm\", how=\"left\") \\\n",
    "               .merge(grp_info, on=\"group_id\", how=\"left\")\n",
    "\n",
    "    # 5) salida final con headers pedidos\n",
    "    out = (pool[[\"ADVERTISER_ADMETRICS_STR\",\"BRAND_ADMETRICS_STR\",\"Match\",\"anunciante_suggest\"]]\n",
    "           .drop_duplicates()\n",
    "           .sort_values([\"anunciante_suggest\",\"BRAND_ADMETRICS_STR\"])\n",
    "           .reset_index(drop=True))\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "805939d3-97d8-4bc0-9416-0b53cf1ee682",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ya tenés: nostandar_df (CSV) y mapping_df (cruce)\n",
    "ai_input_df = build_ai_input_table(nostandar_df, mapping_df, threshold=90)\n",
    "# ai_input_df tiene: ADVERTISER_ADMETRICS_STR | BRAND_ADMETRICS_STR | Match | anunciante_suggest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f3e770f3-3fef-46dc-af90-3a67a08a1938",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ai_input_df   #Asignarle un id a los que se parecen   -- > linkedin \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "827ae83f-df9c-4c29-a1b1-6829681e0036",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80f7c110-f68c-4090-8509-cd36bcaf890e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GEMINI_API_KEY\"] = \"AIzaSyBfwHdzUv6wHL2hMXxo46hCqOnyCogoNG4\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d66d4cc7-0707-47bf-bb8c-056472931a01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === IA SOLO PARA NO MATCHEADOS DEL mapping_df ===\n",
    "import os, json, time, re, pandas as pd\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "# --- Requisitos ---\n",
    "assert os.getenv(\"GEMINI_API_KEY\"), \"Falta GEMINI_API_KEY (export GEMINI_API_KEY=...)\"\n",
    "MODEL = \"gemini-2.5-flash\"\n",
    "BATCH_SIZE = 10     # como pediste\n",
    "RPM = 8             # free tier ~10 rpm\n",
    "\n",
    "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "# --- Qué filas NO matchearon ---\n",
    "# Criterios típicos: match_scope == 'none'  o  decision == 'NUEVO/REVISAR'  o  coincidencia == 'SIN MATCH'\n",
    "def _is_unmatched(row):\n",
    "    ms = str(row.get(\"match_scope\",\"\")).strip().lower()\n",
    "    dc = str(row.get(\"decision\",\"\")).strip().upper()\n",
    "    cc = str(row.get(\"coincidencia\",\"\")).strip().upper()\n",
    "    return (ms == \"none\") or (dc == \"NUEVO/REVISAR\") or (cc == \"SIN MATCH\")\n",
    "\n",
    "# --- Normalizador simple (solo para hints, IA entiende texto crudo igual) ---\n",
    "def _norm(s:str)->str:\n",
    "    import unicodedata\n",
    "    s = unicodedata.normalize('NFKD', str(s or '').lower())\n",
    "    s = ''.join(ch for ch in s if not unicodedata.combining(ch))\n",
    "    s = re.sub(r'\\b(s\\.?a\\.?s?|s\\.?a\\.?|s\\.?r\\.?l\\.?|ltda|ltd|llc|inc|corp|gmbh|s\\.? de r\\.?l\\.?)\\b','',s)\n",
    "    s = re.sub(r'[^a-z0-9 ]+',' ',s)\n",
    "    return re.sub(r'\\s+',' ',s).strip()\n",
    "\n",
    "# --- Prompt de la lógica acordada ---\n",
    "SYSTEM = \"\"\"Eres un resolutor de entidades publicitarias para LATAM.\n",
    "Política:\n",
    "- BRAND es la marca hija/comercial a estandarizar (puede venir mal escrita/variantes).\n",
    "- ADVERTISER es la marca madre/holding (compañía).\n",
    "- Si puedes inferir la madre a partir de la hija, hazlo (Instagram→Meta Platforms, Sprite→The Coca-Cola Company, Mercado Pago→Mercado Libre).\n",
    "- Si no hay evidencia suficiente para la madre, usa \"unknown\".\n",
    "Responde SOLO lista JSON con objetos:\n",
    "{\"id\":<int>,\"brand_canonico\":\"<string>\",\"advertiser_canonico\":\"<string|unknown>\",\"confidence\":<0..1>,\"reason\":\"<breve>\"}\"\"\"\n",
    "\n",
    "def _build_prompt(batch_cases):\n",
    "    return f\"\"\"{SYSTEM}\n",
    "\n",
    "Entrada:\n",
    "{json.dumps(batch_cases, ensure_ascii=False)}\n",
    "\n",
    "Salida (mismo orden):\n",
    "[{{\"id\":<int>,\"brand_canonico\":\"<string>\",\"advertiser_canonico\":\"<string|unknown>\",\"confidence\":<0..1>,\"reason\":\"<breve>\"}}]\"\"\"\n",
    "\n",
    "def _call_gemini_batches(cases, batch_size=BATCH_SIZE, rpm=RPM):\n",
    "    out = []\n",
    "    min_interval = 60.0/max(1,rpm)\n",
    "    last = 0.0\n",
    "    for i in range(0, len(cases), batch_size):\n",
    "        batch = cases[i:i+batch_size]\n",
    "        # pacing\n",
    "        now = time.time(); wait = min_interval - (now - last)\n",
    "        if wait > 0: time.sleep(wait)\n",
    "        last = time.time()\n",
    "\n",
    "        prompt = _build_prompt(batch)\n",
    "        resp = client.models.generate_content(\n",
    "            model=MODEL,\n",
    "            contents=prompt,\n",
    "            config=types.GenerateContentConfig(\n",
    "                temperature=0.0,\n",
    "                thinking_config=types.ThinkingConfig(thinking_budget=0)\n",
    "            ),\n",
    "        )\n",
    "        txt = resp.text or \"[]\"\n",
    "        try:\n",
    "            data = json.loads(txt)\n",
    "        except:\n",
    "            i0, j0 = txt.find(\"[\"), txt.rfind(\"]\")+1\n",
    "            data = json.loads(txt[i0:j0]) if 0<=i0<j0 else []\n",
    "        out.extend(data)\n",
    "    return out\n",
    "\n",
    "def run_ia_on_unmatched(mapping_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = mapping_df.copy()\n",
    "    # 1) Filtrar no matcheados\n",
    "    mask_unmatched = df.apply(_is_unmatched, axis=1)\n",
    "    df_unmatched = df.loc[mask_unmatched].copy()\n",
    "\n",
    "    if df_unmatched.empty:\n",
    "        # nada para IA, devolvemos agregando columnas vacías\n",
    "        for c in [\"brand_canonico_ia\",\"advertiser_canonico_ia\",\"conf_ia\",\"razon_ia\",\"metodo_ia\",\"needs_review\"]:\n",
    "            if c not in df.columns: df[c] = \"\" if c not in (\"conf_ia\",\"needs_review\") else (0.0 if c==\"conf_ia\" else False)\n",
    "        return df\n",
    "\n",
    "    # 2) Armar casos (brand y advertiser crudos del CSV / columnas que tenés)\n",
    "    # columnas fuente (de tu ejemplo):\n",
    "    # - admetrics_brand        -> brand de CSV\n",
    "    # - advertiser_admetrics   -> advertiser de CSV\n",
    "    brand_src = \"admetrics_brand\" if \"admetrics_brand\" in df_unmatched.columns else \"brand_str\"\n",
    "    adv_src   = \"advertiser_admetrics\" if \"advertiser_admetrics\" in df_unmatched.columns else \"advertiser_str\"\n",
    "\n",
    "    subset = df_unmatched[[brand_src, adv_src]].drop_duplicates().reset_index(drop=True)\n",
    "    subset = subset.reset_index().rename(columns={\"index\":\"_id\"})\n",
    "\n",
    "    cases = [{\n",
    "        \"id\": int(r[\"_id\"]),\n",
    "        \"brand_raw\": str(r[brand_src]),\n",
    "        \"advertiser_raw\": str(r[adv_src]),\n",
    "        \"brand_hint\": _norm(r[brand_src]),\n",
    "        \"advertiser_hint\": _norm(r[adv_src])\n",
    "    } for _, r in subset.iterrows()]\n",
    "\n",
    "    # 3) Llamar IA en lotes de 10\n",
    "    ia_res = _call_gemini_batches(cases, batch_size=BATCH_SIZE, rpm=RPM)\n",
    "    ia_df = pd.DataFrame(ia_res).rename(columns={\n",
    "        \"brand_canonico\":\"brand_canonico_ia\",\n",
    "        \"advertiser_canonico\":\"advertiser_canonico_ia\",\n",
    "        \"confidence\":\"conf_ia\",\n",
    "        \"reason\":\"razon_ia\",\n",
    "        \"id\":\"_id\"\n",
    "    })\n",
    "\n",
    "    # 4) Escribir resultados IA en las filas no matcheadas\n",
    "    df_ia = subset.merge(ia_df, on=\"_id\", how=\"left\")\n",
    "    # join por (brand_raw, advertiser_raw)\n",
    "    df = df.merge(\n",
    "        df_ia[[brand_src, adv_src, \"brand_canonico_ia\",\"advertiser_canonico_ia\",\"conf_ia\",\"razon_ia\"]],\n",
    "        on=[brand_src, adv_src],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # 5) Marcas finales y flags\n",
    "    df[\"metodo_ia\"] = \"\"\n",
    "    df.loc[mask_unmatched & df[\"brand_canonico_ia\"].notna(), \"metodo_ia\"] = \"ia\"\n",
    "    df[\"conf_ia\"] = df[\"conf_ia\"].fillna(0.0).astype(float)\n",
    "    # flag de revisión si conf < 0.75\n",
    "    df[\"needs_review\"] = False\n",
    "    df.loc[mask_unmatched & (df[\"conf_ia\"] < 0.75), \"needs_review\"] = True\n",
    "\n",
    "    # 6) Dejar limpio NaN -> ''\n",
    "    for c in [\"brand_canonico_ia\",\"advertiser_canonico_ia\",\"razon_ia\",\"metodo_ia\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].fillna(\"\")\n",
    "    return df\n",
    "\n",
    "print(\"✔️ IA para no matcheados lista (batch=10, rpm<=8)\")\n",
    "\n",
    "# === EJECUCIÓN (usa tu mapping_df ya armado) ===\n",
    "# resultado_df = run_ia_on_unmatched(mapping_df)\n",
    "# display(resultado_df.head(30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb6e5262-d795-4375-a709-ff3f8bd4482b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_1=mapping_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5911144e-8460-475d-ae23-eca7b3029529",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "resultado_df = run_ia_on_unmatched(test_1)\n",
    "display(resultado_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d088a64-d148-44cf-8124-ac4fd91af037",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "resultado_df.to_excel(\"ulrimo_res_ia.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb5a9485-4182-441e-878b-8aebdde57bf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Segundo prompt- check como funciona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9ac6c38-283f-418f-907c-4c08176a8c61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === IA SOLO PARA NO MATCHEADOS DEL mapping_df ===\n",
    "import os, json, time, re, pandas as pd\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "# --- Requisitos ---\n",
    "assert os.getenv(\"GEMINI_API_KEY\"), \"Falta GEMINI_API_KEY (export GEMINI_API_KEY=...)\"\n",
    "MODEL = \"gemini-2.5-flash\"\n",
    "BATCH_SIZE = 8    # como pediste\n",
    "RPM = 6             # free tier ~10 rpm\n",
    "\n",
    "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "# --- Qué filas NO matchearon ---\n",
    "# Criterios típicos: match_scope == 'none'  o  decision == 'NUEVO/REVISAR'  o  coincidencia == 'SIN MATCH'\n",
    "def _is_unmatched(row):\n",
    "    ms = str(row.get(\"match_scope\",\"\")).strip().lower()\n",
    "    dc = str(row.get(\"decision\",\"\")).strip().upper()\n",
    "    cc = str(row.get(\"coincidencia\",\"\")).strip().upper()\n",
    "    return (ms == \"none\") or (dc == \"NUEVO/REVISAR\") or (cc == \"SIN MATCH\")\n",
    "\n",
    "# --- Normalizador simple (solo para hints, IA entiende texto crudo igual) ---\n",
    "def _norm(s:str)->str:\n",
    "    import unicodedata\n",
    "    s = unicodedata.normalize('NFKD', str(s or '').lower())\n",
    "    s = ''.join(ch for ch in s if not unicodedata.combining(ch))\n",
    "    s = re.sub(r'\\b(s\\.?a\\.?s?|s\\.?a\\.?|s\\.?r\\.?l\\.?|ltda|ltd|llc|inc|corp|gmbh|s\\.? de r\\.?l\\.?)\\b','',s)\n",
    "    s = re.sub(r'[^a-z0-9 ]+',' ',s)\n",
    "    return re.sub(r'\\s+',' ',s).strip()\n",
    "\n",
    "# --- Prompt de la lógica acordada ---\n",
    "SYSTEM = \"\"\"Eres un resolutor de entidades publicitarias para LATAM.\n",
    "Política:\n",
    "- BRAND es la marca hija/comercial a estandarizar (puede venir mal escrita/variantes).\n",
    "- ADVERTISER es la marca madre/holding (compañía).\n",
    "- Si la marca madre de entrada es 'Not classified' o 'unknown', **DEBES** inferir y buscar la compañía matriz (ADVERTISER) a partir de la marca hija (BRAND).\n",
    "- Si no hay evidencia suficiente para la madre, usa \"unknown\".\n",
    "Responde SOLO lista JSON con objetos:\n",
    "{\"id\":<int>,\"brand_canonico\":\"<string>\",\"advertiser_canonico\":\"<string|unknown>\",\"confidence\":<0..1>,\"reason\":\"<breve>\"}\"\"\"\n",
    "\n",
    "def _build_prompt(batch_cases):\n",
    "    return f\"\"\"{SYSTEM}\n",
    "\n",
    "Entrada:\n",
    "{json.dumps(batch_cases, ensure_ascii=False)}\n",
    "\n",
    "Salida (mismo orden):\n",
    "[{{\"id\":<int>,\"brand_canonico\":\"<string>\",\"advertiser_canonico\":\"<string|unknown>\",\"confidence\":<0..1>,\"reason\":\"<breve>\"}}]\"\"\"\n",
    "\n",
    "def _call_gemini_batches(cases, batch_size=BATCH_SIZE, rpm=RPM):\n",
    "    out = []\n",
    "    min_interval = 60.0/max(1,rpm)\n",
    "    last = 0.0\n",
    "    for i in range(0, len(cases), batch_size):\n",
    "        batch = cases[i:i+batch_size]\n",
    "        # pacing\n",
    "        now = time.time(); wait = min_interval - (now - last)\n",
    "        if wait > 0: time.sleep(wait)\n",
    "        last = time.time()\n",
    "\n",
    "        prompt = _build_prompt(batch)\n",
    "        resp = client.models.generate_content(\n",
    "            model=MODEL,\n",
    "            contents=prompt,\n",
    "            config=types.GenerateContentConfig(\n",
    "                temperature=0.0,\n",
    "                thinking_config=types.ThinkingConfig(thinking_budget=0)\n",
    "            ),\n",
    "        )\n",
    "        txt = resp.text or \"[]\"\n",
    "        try:\n",
    "            data = json.loads(txt)\n",
    "        except:\n",
    "            i0, j0 = txt.find(\"[\"), txt.rfind(\"]\")+1\n",
    "            data = json.loads(txt[i0:j0]) if 0<=i0<j0 else []\n",
    "        out.extend(data)\n",
    "    return out\n",
    "\n",
    "def run_ia_on_unmatched(mapping_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = mapping_df.copy()\n",
    "    # 1) Filtrar no matcheados\n",
    "    mask_unmatched = df.apply(_is_unmatched, axis=1)\n",
    "    df_unmatched = df.loc[mask_unmatched].copy()\n",
    "\n",
    "    if df_unmatched.empty:\n",
    "        # nada para IA, devolvemos agregando columnas vacías\n",
    "        for c in [\"brand_canonico_ia\",\"advertiser_canonico_ia\",\"conf_ia\",\"razon_ia\",\"metodo_ia\",\"needs_review\"]:\n",
    "            if c not in df.columns: df[c] = \"\" if c not in (\"conf_ia\",\"needs_review\") else (0.0 if c==\"conf_ia\" else False)\n",
    "        return df\n",
    "\n",
    "    # 2) Armar casos (brand y advertiser crudos del CSV / columnas que tenés)\n",
    "    # columnas fuente (de tu ejemplo):\n",
    "    # - admetrics_brand        -> brand de CSV\n",
    "    # - advertiser_admetrics   -> advertiser de CSV\n",
    "    brand_src = \"admetrics_brand\" if \"admetrics_brand\" in df_unmatched.columns else \"brand_str\"\n",
    "    adv_src   = \"advertiser_admetrics\" if \"advertiser_admetrics\" in df_unmatched.columns else \"advertiser_str\"\n",
    "\n",
    "    subset = df_unmatched[[brand_src, adv_src]].drop_duplicates().reset_index(drop=True)\n",
    "    subset = subset.reset_index().rename(columns={\"index\":\"_id\"})\n",
    "\n",
    "    cases = [{\n",
    "        \"id\": int(r[\"_id\"]),\n",
    "        \"brand_raw\": str(r[brand_src]),\n",
    "        \"advertiser_raw\": str(r[adv_src]),\n",
    "        \"brand_hint\": _norm(r[brand_src]),\n",
    "        \"advertiser_hint\": _norm(r[adv_src])\n",
    "    } for _, r in subset.iterrows()]\n",
    "\n",
    "    # 3) Llamar IA en lotes de 10\n",
    "    ia_res = _call_gemini_batches(cases, batch_size=BATCH_SIZE, rpm=RPM)\n",
    "    ia_df = pd.DataFrame(ia_res).rename(columns={\n",
    "        \"brand_canonico\":\"brand_canonico_ia\",\n",
    "        \"advertiser_canonico\":\"advertiser_canonico_ia\",\n",
    "        \"confidence\":\"conf_ia\",\n",
    "        \"reason\":\"razon_ia\",\n",
    "        \"id\":\"_id\"\n",
    "    })\n",
    "\n",
    "    # 4) Escribir resultados IA en las filas no matcheadas\n",
    "    df_ia = subset.merge(ia_df, on=\"_id\", how=\"left\")\n",
    "    # join por (brand_raw, advertiser_raw)\n",
    "    df = df.merge(\n",
    "        df_ia[[brand_src, adv_src, \"brand_canonico_ia\",\"advertiser_canonico_ia\",\"conf_ia\",\"razon_ia\"]],\n",
    "        on=[brand_src, adv_src],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # 5) Marcas finales y flags\n",
    "    df[\"metodo_ia\"] = \"\"\n",
    "    df.loc[mask_unmatched & df[\"brand_canonico_ia\"].notna(), \"metodo_ia\"] = \"ia\"\n",
    "    df[\"conf_ia\"] = df[\"conf_ia\"].fillna(0.0).astype(float)\n",
    "    # flag de revisión si conf < 0.75\n",
    "    df[\"needs_review\"] = False\n",
    "    df.loc[mask_unmatched & (df[\"conf_ia\"] < 0.75), \"needs_review\"] = True\n",
    "\n",
    "    # 6) Dejar limpio NaN -> ''\n",
    "    for c in [\"brand_canonico_ia\",\"advertiser_canonico_ia\",\"razon_ia\",\"metodo_ia\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].fillna(\"\")\n",
    "    return df\n",
    "\n",
    "print(\"✔️ IA para no matcheados lista (batch=10, rpm<=8)\")\n",
    "\n",
    "# === EJECUCIÓN (usa tu mapping_df ya armado) ===\n",
    "# resultado_df = run_ia_on_unmatched(mapping_df)\n",
    "# display(resultado_df.head(30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f61a7102-c02c-42c5-8aef-fdb34d114228",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "resultado_df = run_ia_on_unmatched(test_1)\n",
    "# display(resultado_df.head(30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c08b870-b628-43e3-a79a-dc38dfe74a70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "resultado_df.to_excel(\"ulrimo_res_ia2.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00a3a45a-c86c-4c60-ab0b-a8c0ecf80e65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b00eb57d-4d91-4142-8ff7-94c36c1c42f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os, json, pandas as pd\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "# Usa la key que ya seteaste en GEMINI_API_KEY\n",
    "assert os.getenv(\"GEMINI_API_KEY\"), \"Falta GEMINI_API_KEY\"\n",
    "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "MODEL = \"gemini-2.5-flash\"\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"Eres un resolutor de entidades publicitarias para LATAM.\n",
    "Devuelve SOLO JSON:\n",
    "{\"empresa_canonica\":\"<string|unknown>\",\"brand_canonico\":\"<string>\",\"confidence\":<0..1>,\"reason\":\"<breve>\"}\"\"\"\n",
    "\n",
    "FEW_SHOTS = [\n",
    "  {\"in\":{\"advertiser\":\"Not classified\",\"brand\":\"Mer Libre - ML\",\"suggest\":\"123 Comprou\"},\n",
    "   \"out\":{\"empresa_canonica\":\"Mercado Libre\",\"brand_canonico\":\"Mercado Libre\",\"confidence\":0.93,\"reason\":\"Alias ML\"}},\n",
    "  {\"in\":{\"advertiser\":\"Not classified\",\"brand\":\"mercadolibre\",\"suggest\":\"Mercado Livre\"},\n",
    "   \"out\":{\"empresa_canonica\":\"Mercado Libre\",\"brand_canonico\":\"Mercado Livre\",\"confidence\":0.91,\"reason\":\"PT-BR\"}},\n",
    "  {\"in\":{\"advertiser\":\"Not classified\",\"brand\":\"zoho\",\"suggest\":\"Zoho\"},\n",
    "   \"out\":{\"empresa_canonica\":\"Zoho Corporation\",\"brand_canonico\":\"Zoho\",\"confidence\":0.92,\"reason\":\"SaaS conocida\"}},\n",
    "  {\"in\":{\"advertiser\":\"Not classified\",\"brand\":\"ACME S.A.S.\",\"suggest\":\"\"},\n",
    "   \"out\":{\"empresa_canonica\":\"unknown\",\"brand_canonico\":\"\",\"confidence\":0.0,\"reason\":\"genérico\"}}\n",
    "]\n",
    "\n",
    "def _build_prompt(advertiser:str, brand:str, suggest:str):\n",
    "    shots = \"\\n\".join([json.dumps({\"input\":s[\"in\"],\"output\":s[\"out\"]}, ensure_ascii=False) for s in FEW_SHOTS])\n",
    "    payload = {\"advertiser\": advertiser or \"\", \"brand\": brand or \"\", \"suggest\": suggest or \"\"}\n",
    "    return f\"\"\"{SYSTEM_PROMPT}\n",
    "\n",
    "Ejemplos:\n",
    "{shots}\n",
    "\n",
    "Caso:\n",
    "{json.dumps(payload, ensure_ascii=False)}\n",
    "\n",
    "Responde SOLO el JSON pedido:\n",
    "{{\"empresa_canonica\":\"<string|unknown>\",\"brand_canonico\":\"<string>\",\"confidence\":<0..1>,\"reason\":\"<breve>\"}}\"\"\"\n",
    "\n",
    "def resolve_ai(ai_input_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    cols = [\"ADVERTISER_ADMETRICS_STR\",\"BRAND_ADMETRICS_STR\",\"anunciante_suggest\"]\n",
    "    df = ai_input_df.copy()\n",
    "    for c in cols:\n",
    "        if c not in df.columns: df[c] = \"\"\n",
    "    df[cols] = df[cols].fillna(\"\")\n",
    "    uniq = df[cols].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    outs = []\n",
    "    for _, r in uniq.iterrows():\n",
    "        prompt = _build_prompt(r[\"ADVERTISER_ADMETRICS_STR\"], r[\"BRAND_ADMETRICS_STR\"], r[\"anunciante_suggest\"])\n",
    "        resp = client.models.generate_content(\n",
    "            model=MODEL,\n",
    "            contents=prompt,\n",
    "            config=types.GenerateContentConfig(temperature=0.0, thinking_config=types.ThinkingConfig(thinking_budget=0)),\n",
    "        )\n",
    "        txt = resp.text or \"\"\n",
    "        try:\n",
    "            data = json.loads(txt)\n",
    "        except:\n",
    "            i, j = txt.find(\"{\"), txt.rfind(\"}\") + 1\n",
    "            data = json.loads(txt[i:j]) if i>=0 and j>i else {\"empresa_canonica\":\"unknown\",\"brand_canonico\":\"\",\"confidence\":0.0,\"reason\":\"parse_error\"}\n",
    "        outs.append({**r.to_dict(),\n",
    "                     \"empresa_canonica_ai\": data.get(\"empresa_canonica\",\"\"),\n",
    "                     \"brand_canonico_ai\":   data.get(\"brand_canonico\",\"\"),\n",
    "                     \"confianza_ai\":        float(data.get(\"confidence\",0.0)),\n",
    "                     \"razon_ai\":            data.get(\"reason\",\"\")})\n",
    "    res = pd.DataFrame(outs)\n",
    "    return (df.merge(res, on=cols, how=\"left\")\n",
    "              [[\"ADVERTISER_ADMETRICS_STR\",\"BRAND_ADMETRICS_STR\",\"anunciante_suggest\",\n",
    "                \"empresa_canonica_ai\",\"brand_canonico_ai\",\"confianza_ai\",\"razon_ai\"]]\n",
    "              .fillna(\"\"))\n",
    "\n",
    "print(\"✔️ resolve_ai definido\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a4401f9-ad95-4379-95cf-de8cc0a58927",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Si tu ai_input_df es Spark: ai_input_df = ai_input_df.toPandas()\n",
    "resultado = resolve_ai(ai_input_df)\n",
    "display(resultado)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84998602-f134-47b7-87d5-a766eccbe188",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os, json, time, re, pandas as pd\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "# --- Config ---\n",
    "assert os.getenv(\"GEMINI_API_KEY\"), \"Falta GEMINI_API_KEY\"\n",
    "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "MODEL = \"gemini-2.5-flash\"\n",
    "\n",
    "# límites conservadores para no chocar el free tier (10 rpm)\n",
    "BATCH_SIZE = 25         # cuántas filas resuelve por request\n",
    "REQUESTS_PER_MIN = 8    # <=10\n",
    "_MIN_INTERVAL = 60.0 / REQUESTS_PER_MIN\n",
    "_last_call_ts = [0.0]\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"Eres un resolutor de entidades publicitarias para LATAM.\n",
    "Debes devolver solo JSON válido. Si no es claro, usa \"unknown\".\n",
    "Esquema por caso: {\"id\": <int>, \"empresa_canonica\":\"<string|unknown>\",\n",
    "\"brand_canonico\":\"<string>\", \"confidence\": <0..1>, \"reason\":\"<breve>\"}\"\"\"\n",
    "\n",
    "def _pace():\n",
    "    \"\"\"Respeta RPM para evitar 429.\"\"\"\n",
    "    now = time.time()\n",
    "    wait = _MIN_INTERVAL - (now - _last_call_ts[0])\n",
    "    if wait > 0:\n",
    "        time.sleep(wait)\n",
    "    _last_call_ts[0] = time.time()\n",
    "\n",
    "def _extract_retry_delay_secs(err_text:str) -> float|None:\n",
    "    m = re.search(r\"retryDelay['\\\":\\s]+(\\d+)s\", err_text)\n",
    "    return float(m.group(1)) if m else None\n",
    "\n",
    "def _extract_json(s:str):\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except:\n",
    "        i, j = s.find(\"[\"), s.rfind(\"]\") + 1   # esperamos una LISTA JSON\n",
    "        if 0 <= i < j:\n",
    "            return json.loads(s[i:j])\n",
    "        i, j = s.find(\"{\"), s.rfind(\"}\") + 1   # fallback objeto\n",
    "        return json.loads(s[i:j]) if 0 <= i < j else None\n",
    "\n",
    "def _build_batch_prompt(cases:list[dict]) -> str:\n",
    "    \"\"\"\n",
    "    cases: [{\"id\":int,\"advertiser\":str,\"brand\":str,\"suggest\":str}, ...]\n",
    "    \"\"\"\n",
    "    return f\"\"\"{SYSTEM_PROMPT}\n",
    "\n",
    "Resuelve múltiples casos a la vez. Mantén el MISMO ORDEN.\n",
    "Entrada (lista JSON):\n",
    "{json.dumps(cases, ensure_ascii=False)}\n",
    "\n",
    "Responde SOLO una lista JSON con el mismo orden:\n",
    "[{{\"id\": <int>, \"empresa_canonica\":\"<string|unknown>\",\n",
    "   \"brand_canonico\":\"<string>\", \"confidence\": <0..1>, \"reason\":\"<breve>\"}}, ...]\"\"\"\n",
    "\n",
    "def _call_gemini_batch(cases:list[dict], max_retries:int=6) -> list[dict]:\n",
    "    prompt = _build_batch_prompt(cases)\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            _pace()  # respeta RPM\n",
    "            resp = client.models.generate_content(\n",
    "                model=MODEL,\n",
    "                contents=prompt,\n",
    "                config=types.GenerateContentConfig(\n",
    "                    temperature=0.0,\n",
    "                    thinking_config=types.ThinkingConfig(thinking_budget=0)\n",
    "                ),\n",
    "            )\n",
    "            data = _extract_json(resp.text or \"\")\n",
    "            if not isinstance(data, list):\n",
    "                raise ValueError(\"Respuesta no es lista JSON\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            txt = str(e)\n",
    "            # Si es 429, dormimos según retryDelay o backoff exponencial\n",
    "            if \"429\" in txt or \"RESOURCE_EXHAUSTED\" in txt:\n",
    "                wait = _extract_retry_delay_secs(txt) or min(60, 2 ** attempt * 2)\n",
    "                time.sleep(wait)\n",
    "                continue\n",
    "            # otros errores: backoff corto y reintento\n",
    "            time.sleep(min(30, 2 ** attempt))\n",
    "    # Si no se pudo, devolvemos unknown para todos\n",
    "    return [{\"id\": c[\"id\"], \"empresa_canonica\":\"unknown\", \"brand_canonico\":\"\", \"confidence\":0.0, \"reason\":\"retry_failed\"} for c in cases]\n",
    "\n",
    "def resolve_ai_batched(ai_input_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # columnas esperadas\n",
    "    cols = [\"ADVERTISER_ADMETRICS_STR\",\"BRAND_ADMETRICS_STR\",\"anunciante_suggest\"]\n",
    "    df = ai_input_df.copy()\n",
    "    for c in cols:\n",
    "        if c not in df.columns: df[c] = \"\"\n",
    "    df[cols] = df[cols].fillna(\"\")\n",
    "\n",
    "    # dedup por combinación para ahorrar llamadas\n",
    "    uniq = df[cols].drop_duplicates().reset_index(drop=True)\n",
    "    # agregamos id para mapear resultados\n",
    "    uniq = uniq.reset_index().rename(columns={\"index\":\"_id\"})\n",
    "\n",
    "    # armar casos\n",
    "    cases_all = [{\n",
    "        \"id\": int(r[\"_id\"]),\n",
    "        \"advertiser\": r[\"ADVERTISER_ADMETRICS_STR\"],\n",
    "        \"brand\": r[\"BRAND_ADMETRICS_STR\"],\n",
    "        \"suggest\": r[\"anunciante_suggest\"]\n",
    "    } for _, r in uniq.iterrows()]\n",
    "\n",
    "    # procesar en lotes\n",
    "    results = []\n",
    "    for i in range(0, len(cases_all), BATCH_SIZE):\n",
    "        batch = cases_all[i:i+BATCH_SIZE]\n",
    "        results.extend(_call_gemini_batch(batch))\n",
    "\n",
    "    # resultados a DataFrame por id\n",
    "    res_df = pd.DataFrame(results).rename(columns={\n",
    "        \"empresa_canonica\":\"empresa_canonica_ai\",\n",
    "        \"brand_canonico\":\"brand_canonico_ai\",\n",
    "        \"confidence\":\"confianza_ai\",\n",
    "        \"reason\":\"razon_ai\",\n",
    "        \"id\":\"_id\"\n",
    "    })\n",
    "    # merge por id -> merge por columnas originales\n",
    "    merged = uniq.merge(res_df, on=\"_id\", how=\"left\").drop(columns=[\"_id\"])\n",
    "    out = df.merge(merged, on=cols, how=\"left\").fillna(\"\")\n",
    "    return out[cols + [\"empresa_canonica_ai\",\"brand_canonico_ai\",\"confianza_ai\",\"razon_ai\"]]\n",
    "\n",
    "print(\"✔️ resolve_ai_batched listo (lotes + throttling + reintentos)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b0c2833-467a-423a-a79d-e55b4eff2ce5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# si es Spark: ai_input_df = ai_input_sdf.toPandas()\n",
    "resultado = resolve_ai_batched(ai_input_df)\n",
    "display(resultado)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "996c7ac1-9bca-403e-9321-22600c21c8dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "resultado.to_excel('resultado_ia.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e1b6c42-dc3a-42d5-a175-eea1a30761a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os, json, time, re, pandas as pd\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "# --- Config ---\n",
    "assert os.getenv(\"GEMINI_API_KEY\"), \"Falta GEMINI_API_KEY\"\n",
    "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "MODEL = \"gemini-2.5-flash\"\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"Eres un resolutor de entidades publicitarias para LATAM.\n",
    "Debes devolver solo JSON válido. Si no es claro, usa \"unknown\".\n",
    "Esquema por caso: {\"id\": <int>, \"empresa_canonica\":\"<string|unknown>\",\n",
    "\"brand_canonico\":\"<string>\", \"confidence\": <0..1>, \"reason\":\"<breve>\"}\"\"\"\n",
    "\n",
    "def _extract_retry_delay_secs(err_text:str):\n",
    "    m = re.search(r\"retryDelay['\\\":\\s]+(\\d+)s\", err_text)\n",
    "    return float(m.group(1)) if m else None\n",
    "\n",
    "def _extract_json(s:str):\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except:\n",
    "        i, j = s.find(\"[\"), s.rfind(\"]\") + 1\n",
    "        if 0 <= i < j: return json.loads(s[i:j])\n",
    "        i, j = s.find(\"{\"), s.rfind(\"}\") + 1\n",
    "        return json.loads(s[i:j]) if 0 <= i < j else None\n",
    "\n",
    "def _build_batch_prompt(cases:list) -> str:\n",
    "    return f\"\"\"{SYSTEM_PROMPT}\n",
    "\n",
    "Resuelve múltiples casos a la vez. Mantén el MISMO ORDEN.\n",
    "Entrada (lista JSON):\n",
    "{json.dumps(cases, ensure_ascii=False)}\n",
    "\n",
    "Responde SOLO una lista JSON con el mismo orden:\n",
    "[{{\"id\": <int>, \"empresa_canonica\":\"<string|unknown>\",\n",
    "   \"brand_canonico\":\"<string>\", \"confidence\": <0..1>, \"reason\":\"<breve>\"}}, ...]\"\"\"\n",
    "\n",
    "def _call_gemini_batch(cases:list, rpm:int=8, max_retries:int=6):\n",
    "    # throttle simple por RPM (free tier ~10 rpm)\n",
    "    min_interval = 60.0 / max(1, rpm)\n",
    "    # pacing\n",
    "    if not hasattr(_call_gemini_batch, \"_last\"): _call_gemini_batch._last = 0.0\n",
    "    now = time.time(); wait = min_interval - (now - _call_gemini_batch._last)\n",
    "    if wait > 0: time.sleep(wait)\n",
    "    _call_gemini_batch._last = time.time()\n",
    "\n",
    "    prompt = _build_batch_prompt(cases)\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            resp = client.models.generate_content(\n",
    "                model=MODEL,\n",
    "                contents=prompt,\n",
    "                config=types.GenerateContentConfig(\n",
    "                    temperature=0.0,\n",
    "                    thinking_config=types.ThinkingConfig(thinking_budget=0)\n",
    "                ),\n",
    "            )\n",
    "            data = _extract_json(resp.text or \"\")\n",
    "            if not isinstance(data, list):\n",
    "                raise ValueError(\"Respuesta no es lista JSON\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            txt = str(e)\n",
    "            if \"429\" in txt or \"RESOURCE_EXHAUSTED\" in txt:\n",
    "                time.sleep(_extract_retry_delay_secs(txt) or min(60, 2 ** attempt * 2))\n",
    "                continue\n",
    "            time.sleep(min(30, 2 ** attempt))\n",
    "    # si falla todo, devolvemos unknown para mantener forma\n",
    "    return [{\"id\": c[\"id\"], \"empresa_canonica\":\"unknown\", \"brand_canonico\":\"\", \"confidence\":0.0, \"reason\":\"retry_failed\"} for c in cases]\n",
    "\n",
    "def resolve_ai_batched(ai_input_df: pd.DataFrame, batch_size:int=10, rpm:int=8) -> pd.DataFrame:\n",
    "    cols = [\"ADVERTISER_ADMETRICS_STR\",\"BRAND_ADMETRICS_STR\",\"anunciante_suggest\"]\n",
    "    df = ai_input_df.copy()\n",
    "    for c in cols:\n",
    "        if c not in df.columns: df[c] = \"\"\n",
    "    df[cols] = df[cols].fillna(\"\")\n",
    "\n",
    "    # dedup por combinación para ahorrar requests\n",
    "    uniq = df[cols].drop_duplicates().reset_index(drop=True)\n",
    "    uniq = uniq.reset_index().rename(columns={\"index\":\"_id\"})  # id para mapear\n",
    "\n",
    "    # armo casos\n",
    "    cases = [{\n",
    "        \"id\": int(r[\"_id\"]),\n",
    "        \"advertiser\": r[\"ADVERTISER_ADMETRICS_STR\"],\n",
    "        \"brand\": r[\"BRAND_ADMETRICS_STR\"],\n",
    "        \"suggest\": r[\"anunciante_suggest\"]\n",
    "    } for _, r in uniq.iterrows()]\n",
    "\n",
    "    # proceso en lotes de 'batch_size'\n",
    "    results = []\n",
    "    total = len(cases)\n",
    "    for i in range(0, total, batch_size):\n",
    "        batch = cases[i:i+batch_size]\n",
    "        print(f\"Lote {i//batch_size + 1}/{(total + batch_size - 1)//batch_size} (size={len(batch)})\")\n",
    "        results.extend(_call_gemini_batch(batch, rpm=rpm))\n",
    "\n",
    "    res_df = pd.DataFrame(results).rename(columns={\n",
    "        \"empresa_canonica\":\"empresa_canonica_ai\",\n",
    "        \"brand_canonico\":\"brand_canonico_ai\",\n",
    "        \"confidence\":\"confianza_ai\",\n",
    "        \"reason\":\"razon_ai\",\n",
    "        \"id\":\"_id\"\n",
    "    })\n",
    "    merged = uniq.merge(res_df, on=\"_id\", how=\"left\").drop(columns=[\"_id\"])\n",
    "    out = df.merge(merged, on=cols, how=\"left\").fillna(\"\")\n",
    "    return out[cols + [\"empresa_canonica_ai\",\"brand_canonico_ai\",\"confianza_ai\",\"razon_ai\"]]\n",
    "\n",
    "print(\"✔️ resolve_ai_batched listo (batch_size=10 por defecto)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe090eaa-6337-48c0-b4b9-81aaa19e66fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "618c89b3-e167-41db-ad2c-d298a19b3b23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "resultado = resolve_ai_batched(ai_input_df, batch_size=10, rpm=8)  # rpm<=10 para free tier\n",
    "# display(resultado.head(30))\n",
    "display(resultado)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e96af8c-5923-428f-ae33-b348897be716",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === IA + WEB SEARCH PARA NO MATCHEADOS ===\n",
    "import os, json, time, re, pandas as pd\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "# --- Requisitos ---\n",
    "assert os.getenv(\"GEMINI_API_KEY\"), \"Falta GEMINI_API_KEY (export GEMINI_API_KEY=...)\"\n",
    "MODEL = \"gemini-2.5-flash\"\n",
    "BATCH_SIZE = 10\n",
    "RPM = 8\n",
    "\n",
    "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "# --- Qué filas NO matchearon ---\n",
    "def _is_unmatched(row):\n",
    "    ms = str(row.get(\"match_scope\",\"\")).strip().lower()\n",
    "    dc = str(row.get(\"decision\",\"\")).strip().upper()\n",
    "    cc = str(row.get(\"coincidencia\",\"\")).strip().upper()\n",
    "    return (ms == \"none\") or (dc == \"NUEVO/REVISAR\") or (cc == \"SIN MATCH\")\n",
    "\n",
    "# --- Normalizador simple ---\n",
    "def _norm(s:str)->str:\n",
    "    import unicodedata\n",
    "    s = unicodedata.normalize('NFKD', str(s or '').lower())\n",
    "    s = ''.join(ch for ch in s if not unicodedata.combining(ch))\n",
    "    s = re.sub(r'\\b(s\\.?a\\.?s?|s\\.?a\\.?|s\\.?r\\.?l\\.?|ltda|ltd|llc|inc|corp|gmbh|s\\.? de r\\.?l\\.?)\\b','',s)\n",
    "    s = re.sub(r'[^a-z0-9 ]+',' ',s)\n",
    "    return re.sub(r'\\s+',' ',s).strip()\n",
    "\n",
    "# --- Prompt mejorado con búsqueda web ---\n",
    "SYSTEM = \"\"\"Eres un resolutor de entidades publicitarias para LATAM con capacidad de búsqueda web.\n",
    "\n",
    "**PROCESO:**\n",
    "1. Para cada caso, PRIMERO usa Google Search para verificar información sobre la marca.\n",
    "2. Busca: \"[brand_raw] marca empresa\", \"[brand_raw] parent company\", \"[brand_raw] holding\"\n",
    "3. Analiza los resultados para identificar:\n",
    "   - Nombre canónico correcto de la marca\n",
    "   - Empresa madre/holding (advertiser)\n",
    "   - Relaciones corporativas\n",
    "\n",
    "**POLÍTICA:**\n",
    "- BRAND: marca hija/comercial a estandarizar (corrige spelling, variantes)\n",
    "- ADVERTISER: marca madre/holding real\n",
    "- Usa información web para confirmar relaciones (ej: Instagram→Meta, Sprite→Coca-Cola Company, Mercado Pago→Mercado Libre)\n",
    "- Si tras búsqueda no hay evidencia clara del holding, usa \"unknown\"\n",
    "- Prioriza fuentes oficiales (sitios corporativos, Wikipedia, LinkedIn)\n",
    "\n",
    "**CONFIANZA:**\n",
    "- 0.9-1.0: Información verificada en fuentes oficiales\n",
    "- 0.7-0.89: Información consistente en múltiples fuentes\n",
    "- 0.5-0.69: Información parcial o fuentes menos confiables\n",
    "- <0.5: Poca evidencia, requiere revisión manual\n",
    "\n",
    "Responde SOLO JSON:\n",
    "[{\"id\":<int>,\"brand_canonico\":\"<string>\",\"advertiser_canonico\":\"<string|unknown>\",\"confidence\":<0..1>,\"reason\":\"<breve con fuentes>\",\"sources_found\":\"<urls principales>\"}]\"\"\"\n",
    "\n",
    "def _build_prompt_with_search(batch_cases):\n",
    "    return f\"\"\"{SYSTEM}\n",
    "\n",
    "**INSTRUCCIÓN IMPORTANTE:** \n",
    "Antes de responder, realiza búsquedas web para cada marca usando Google Search integrado.\n",
    "Queries sugeridas por caso:\n",
    "- \"[brand_raw] empresa\"\n",
    "- \"[brand_raw] parent company latam\"\n",
    "- \"[brand_raw] holding advertiser\"\n",
    "\n",
    "Entrada:\n",
    "{json.dumps(batch_cases, ensure_ascii=False, indent=2)}\n",
    "\n",
    "Salida (mismo orden, con información verificada por web):\n",
    "[{{\"id\":<int>,\"brand_canonico\":\"<string>\",\"advertiser_canonico\":\"<string|unknown>\",\"confidence\":<0..1>,\"reason\":\"<fuente>\",\"sources_found\":\"<urls>\"}}]\"\"\"\n",
    "\n",
    "def _call_gemini_with_search(cases, batch_size=BATCH_SIZE, rpm=RPM):\n",
    "    \"\"\"\n",
    "    Llama a Gemini con capacidad de búsqueda web habilitada.\n",
    "    Gemini 2.5 Flash tiene Google Search integrado cuando se activa.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    min_interval = 60.0/max(1,rpm)\n",
    "    last = 0.0\n",
    "    \n",
    "    for i in range(0, len(cases), batch_size):\n",
    "        batch = cases[i:i+batch_size]\n",
    "        now = time.time()\n",
    "        wait = min_interval - (now - last)\n",
    "        if wait > 0: \n",
    "            time.sleep(wait)\n",
    "        last = time.time()\n",
    "\n",
    "        prompt = _build_prompt_with_search(batch)\n",
    "        \n",
    "        try:\n",
    "            # Habilitar Google Search y Code Execution para máxima capacidad\n",
    "            resp = client.models.generate_content(\n",
    "                model=MODEL,\n",
    "                contents=prompt,\n",
    "                config=types.GenerateContentConfig(\n",
    "                    temperature=0.1,  # Bajo para ser más factual\n",
    "                    tools=[\n",
    "                        types.Tool(google_search=types.GoogleSearch()),\n",
    "                        types.Tool(code_execution=types.CodeExecution())\n",
    "                    ],\n",
    "                    thinking_config=types.ThinkingConfig(thinking_budget=0)\n",
    "                ),\n",
    "            )\n",
    "            \n",
    "            txt = resp.text or \"[]\"\n",
    "            \n",
    "            # Log de búsquedas realizadas (si están disponibles)\n",
    "            if hasattr(resp, 'candidates') and resp.candidates:\n",
    "                for part in resp.candidates[0].content.parts:\n",
    "                    if hasattr(part, 'executable_code'):\n",
    "                        print(f\"🔍 Búsqueda ejecutada en batch {i//batch_size + 1}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error en batch {i//batch_size + 1}: {str(e)}\")\n",
    "            txt = \"[]\"\n",
    "        \n",
    "        # Parse JSON response\n",
    "        try:\n",
    "            data = json.loads(txt)\n",
    "        except:\n",
    "            # Intenta extraer JSON del texto\n",
    "            i0, j0 = txt.find(\"[\"), txt.rfind(\"]\")+1\n",
    "            if 0 <= i0 < j0:\n",
    "                try:\n",
    "                    data = json.loads(txt[i0:j0])\n",
    "                except:\n",
    "                    print(f\"⚠️ No se pudo parsear respuesta del batch {i//batch_size + 1}\")\n",
    "                    data = []\n",
    "            else:\n",
    "                data = []\n",
    "        \n",
    "        out.extend(data)\n",
    "        print(f\"✅ Batch {i//batch_size + 1}/{(len(cases)-1)//batch_size + 1} procesado ({len(data)} resultados)\")\n",
    "    \n",
    "    return out\n",
    "\n",
    "def run_ia_on_unmatched(mapping_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Procesa filas no matcheadas usando IA con búsqueda web.\n",
    "    \"\"\"\n",
    "    df = mapping_df.copy()\n",
    "    \n",
    "    # 1) Filtrar no matcheados\n",
    "    mask_unmatched = df.apply(_is_unmatched, axis=1)\n",
    "    df_unmatched = df.loc[mask_unmatched].copy()\n",
    "\n",
    "    if df_unmatched.empty:\n",
    "        print(\"ℹ️ No hay casos sin matchear, nada que procesar con IA\")\n",
    "        for c in [\"brand_canonico_ia\",\"advertiser_canonico_ia\",\"conf_ia\",\"razon_ia\",\"sources_ia\",\"metodo_ia\",\"needs_review\"]:\n",
    "            if c not in df.columns: \n",
    "                df[c] = \"\" if c not in (\"conf_ia\",\"needs_review\") else (0.0 if c==\"conf_ia\" else False)\n",
    "        return df\n",
    "\n",
    "    print(f\"🔍 Encontrados {len(df_unmatched)} casos sin matchear para procesar con IA + Web Search\")\n",
    "\n",
    "    # 2) Armar casos únicos\n",
    "    brand_src = \"admetrics_brand\" if \"admetrics_brand\" in df_unmatched.columns else \"brand_str\"\n",
    "    adv_src = \"advertiser_admetrics\" if \"advertiser_admetrics\" in df_unmatched.columns else \"advertiser_str\"\n",
    "\n",
    "    subset = df_unmatched[[brand_src, adv_src]].drop_duplicates().reset_index(drop=True)\n",
    "    subset = subset.reset_index().rename(columns={\"index\":\"_id\"})\n",
    "\n",
    "    cases = [{\n",
    "        \"id\": int(r[\"_id\"]),\n",
    "        \"brand_raw\": str(r[brand_src]),\n",
    "        \"advertiser_raw\": str(r[adv_src]),\n",
    "        \"brand_hint\": _norm(r[brand_src]),\n",
    "        \"advertiser_hint\": _norm(r[adv_src])\n",
    "    } for _, r in subset.iterrows()]\n",
    "\n",
    "    print(f\"📊 Total de casos únicos a procesar: {len(cases)}\")\n",
    "\n",
    "    # 3) Llamar IA con búsqueda web\n",
    "    print(f\"🤖 Iniciando procesamiento con Gemini + Google Search (batches de {BATCH_SIZE})...\")\n",
    "    ia_res = _call_gemini_with_search(cases, batch_size=BATCH_SIZE, rpm=RPM)\n",
    "    \n",
    "    ia_df = pd.DataFrame(ia_res).rename(columns={\n",
    "        \"brand_canonico\": \"brand_canonico_ia\",\n",
    "        \"advertiser_canonico\": \"advertiser_canonico_ia\",\n",
    "        \"confidence\": \"conf_ia\",\n",
    "        \"reason\": \"razon_ia\",\n",
    "        \"sources_found\": \"sources_ia\",\n",
    "        \"id\": \"_id\"\n",
    "    })\n",
    "\n",
    "    # 4) Merge resultados\n",
    "    df_ia = subset.merge(ia_df, on=\"_id\", how=\"left\")\n",
    "    df = df.merge(\n",
    "        df_ia[[brand_src, adv_src, \"brand_canonico_ia\",\"advertiser_canonico_ia\",\"conf_ia\",\"razon_ia\",\"sources_ia\"]],\n",
    "        on=[brand_src, adv_src],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # 5) Flags y limpieza\n",
    "    df[\"metodo_ia\"] = \"\"\n",
    "    df.loc[mask_unmatched & df[\"brand_canonico_ia\"].notna(), \"metodo_ia\"] = \"ia_web_search\"\n",
    "    df[\"conf_ia\"] = df[\"conf_ia\"].fillna(0.0).astype(float)\n",
    "    df[\"needs_review\"] = False\n",
    "    df.loc[mask_unmatched & (df[\"conf_ia\"] < 0.75), \"needs_review\"] = True\n",
    "\n",
    "    for c in [\"brand_canonico_ia\",\"advertiser_canonico_ia\",\"razon_ia\",\"sources_ia\",\"metodo_ia\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].fillna(\"\")\n",
    "\n",
    "    # 6) Estadísticas\n",
    "    processed = (df[\"metodo_ia\"] == \"ia_web_search\").sum()\n",
    "    high_conf = ((df[\"conf_ia\"] >= 0.75) & (df[\"metodo_ia\"] == \"ia_web_search\")).sum()\n",
    "    need_review = df[\"needs_review\"].sum()\n",
    "    \n",
    "    print(f\"\\n📈 RESULTADOS:\")\n",
    "    print(f\"  ✅ Procesados con IA: {processed}\")\n",
    "    print(f\"  🎯 Alta confianza (≥0.75): {high_conf} ({high_conf/processed*100:.1f}%)\" if processed > 0 else \"\")\n",
    "    print(f\"  ⚠️  Requieren revisión: {need_review}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "print(\"✨ IA mejorada con Google Search lista\")\n",
    "print(\"📋 Características:\")\n",
    "print(\"  • Búsqueda web automática por cada marca\")\n",
    "print(\"  • Verificación de holdings en tiempo real\")\n",
    "print(\"  • Mayor precisión con fuentes oficiales\")\n",
    "print(\"  • Tracking de fuentes consultadas\")\n",
    "print(\"\\n💡 Uso: resultado_df = run_ia_on_unmatched(mapping_df)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ebd3ae1-1588-49ab-b34c-ffb0ade64c2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Resultado_df = run_ia_on_unmatched(test_1)\n",
    "display(test_1)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "MATCH ADV",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
